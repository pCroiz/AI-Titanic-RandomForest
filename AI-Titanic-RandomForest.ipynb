{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNB8pD6cn5YigNLbP2z+FSj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Let's Import all the libraries needed"],"metadata":{"id":"73agGvzko7dp"}},{"cell_type":"code","execution_count":31,"metadata":{"id":"x558VlMBnmdn","executionInfo":{"status":"ok","timestamp":1721674471146,"user_tz":-120,"elapsed":346,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}}},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_decision_forests as tfdf\n","from keras import layers\n","import pandas as pd\n","import numpy as np\n","import os\n","from google.colab import files"]},{"cell_type":"markdown","source":["First thing first, we need to import the data. Let's dowload it from my GitHub repository"],"metadata":{"id":"fOXua6P7pAMR"}},{"cell_type":"code","source":["### Get the data ###\n","dataTrain = pd.read_csv('https://raw.githubusercontent.com/pCroiz/AI-Titanic-RandomForest/main/data/train.csv')\n","dataTest = pd.read_csv('https://raw.githubusercontent.com/pCroiz/AI-Titanic-RandomForest/main/data/test.csv')"],"metadata":{"id":"I2zvHjoepmfy","executionInfo":{"status":"ok","timestamp":1721672474583,"user_tz":-120,"elapsed":776,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Now let's preprocess the data"],"metadata":{"id":"2haTy_VKrkpu"}},{"cell_type":"code","source":["### Preprocess the data ###\n","print(dataTrain)\n","\n","def preprocess(df):\n","    df = df.copy()\n","\n","    def normalize_name(x):\n","        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n","\n","    def ticket_number(x):\n","        return x.split(\" \")[-1]\n","\n","    def ticket_item(x):\n","        items = x.split(\" \")\n","        if len(items) == 1:\n","            return \"NONE\"\n","        return \"_\".join(items[0:-1])\n","\n","    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n","    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n","    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)\n","\n","    # Error with the Cabin column\n","    df.drop('Cabin', axis=1, inplace=True)\n","\n","    # Error with the Embarked column\n","    df.drop('Embarked', axis=1, inplace=True)\n","\n","    return df\n","\n","preProc_dataTrain = preprocess(dataTrain)\n","preProc_dataTest = preprocess(dataTest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3-iZb7rn73","executionInfo":{"status":"ok","timestamp":1721672568261,"user_tz":-120,"elapsed":292,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}},"outputId":"012b6471-dbf4-4abd-9873-f5d2d56cbbf1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["     PassengerId  Survived  Pclass  \\\n","0              1         0       3   \n","1              2         1       1   \n","2              3         1       3   \n","3              4         1       1   \n","4              5         0       3   \n","..           ...       ...     ...   \n","886          887         0       2   \n","887          888         1       1   \n","888          889         0       3   \n","889          890         1       1   \n","890          891         0       3   \n","\n","                                                  Name     Sex   Age  SibSp  \\\n","0                              Braund, Mr. Owen Harris    male  22.0      1   \n","1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                               Heikkinen, Miss. Laina  female  26.0      0   \n","3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                             Allen, Mr. William Henry    male  35.0      0   \n","..                                                 ...     ...   ...    ...   \n","886                              Montvila, Rev. Juozas    male  27.0      0   \n","887                       Graham, Miss. Margaret Edith  female  19.0      0   \n","888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n","889                              Behr, Mr. Karl Howell    male  26.0      0   \n","890                                Dooley, Mr. Patrick    male  32.0      0   \n","\n","     Parch            Ticket     Fare Cabin Embarked  \n","0        0         A/5 21171   7.2500   NaN        S  \n","1        0          PC 17599  71.2833   C85        C  \n","2        0  STON/O2. 3101282   7.9250   NaN        S  \n","3        0            113803  53.1000  C123        S  \n","4        0            373450   8.0500   NaN        S  \n","..     ...               ...      ...   ...      ...  \n","886      0            211536  13.0000   NaN        S  \n","887      0            112053  30.0000   B42        S  \n","888      2        W./C. 6607  23.4500   NaN        S  \n","889      0            111369  30.0000  C148        C  \n","890      0            370376   7.7500   NaN        Q  \n","\n","[891 rows x 12 columns]\n"]}]},{"cell_type":"markdown","source":["Let's get the Input features"],"metadata":{"id":"hv-8WnzJrvJX"}},{"cell_type":"code","source":["### Get the input features ###\n","\n","features = []\n","removesFeature = [\"Ticket\",\"PassengerId\",\"Survived\"]\n","\n","for key in preProc_dataTrain.keys():\n","    if key != 'Survived':\n","        features.append(tfdf.keras.FeatureUsage(name=key))"],"metadata":{"id":"-z0lSjmyryKk","executionInfo":{"status":"ok","timestamp":1721672943995,"user_tz":-120,"elapsed":267,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Let's define our input function. Usefull to convert pd data to tf data"],"metadata":{"id":"fXpI5XRPr17J"}},{"cell_type":"code","source":["### Definition of the Input Function ###\n","\n","# Usefull to convert the pd dataset to an tf dataset\n","def tokenize_names(features, labels=None):\n","    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n","    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n","    return features, labels\n","\n","train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preProc_dataTrain,label=\"Survived\").map(tokenize_names)\n","test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preProc_dataTest).map(tokenize_names)"],"metadata":{"id":"Ugcc1udQr8DV","executionInfo":{"status":"ok","timestamp":1721673158580,"user_tz":-120,"elapsed":668,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["Now, let's create our model"],"metadata":{"id":"2LCJ1tIDr9ex"}},{"cell_type":"code","source":["model = tfdf.keras.GradientBoostedTreesModel(\n","    verbose=0, # Very few logs\n","    features=features,\n","    exclude_non_specified_features=True, # Only use the features in \"features\"\n","    random_seed=1234,\n",")\n"],"metadata":{"id":"K7zEg9FRr_iV","executionInfo":{"status":"ok","timestamp":1721673204986,"user_tz":-120,"elapsed":323,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Now we format the training dataset and we train/fit the model"],"metadata":{"id":"nezY_sTwtQ9F"}},{"cell_type":"code","source":["model.fit(train_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gIKKe7HwtVes","executionInfo":{"status":"ok","timestamp":1721673225261,"user_tz":-120,"elapsed":18002,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}},"outputId":"33a2e1c6-7a29-40ba-c264-64009c19d996"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf_keras.src.callbacks.History at 0x7f7a1c2fb250>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["Let's evaluate the model"],"metadata":{"id":"w9rOsunwuutO"}},{"cell_type":"code","source":["self_evaluation = model.make_inspector().evaluation()\n","print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dws_RjHeuzbk","executionInfo":{"status":"ok","timestamp":1721673420331,"user_tz":-120,"elapsed":321,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}},"outputId":"8af3ca5b-a108-4763-cba5-d06c7e109587"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8260869383811951 Loss:0.8741847276687622\n"]}]},{"cell_type":"markdown","source":["Let's make the prediction"],"metadata":{"id":"XzV95Xy2vSud"}},{"cell_type":"code","source":["def prediction_to_kaggle_format(model, threshold=0.5):\n","    proba_survive = model.predict(test_ds, verbose=0)[:,0]\n","    return pd.DataFrame({\n","        \"PassengerId\": dataTest[\"PassengerId\"],\n","        \"Survived\": (proba_survive >= threshold).astype(int)\n","    })\n","\n","kaggle_predictions = prediction_to_kaggle_format(model)\n","print(kaggle_predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AP50zKfqvuHl","executionInfo":{"status":"ok","timestamp":1721674064519,"user_tz":-120,"elapsed":413,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}},"outputId":"f48a81fb-9f17-45be-8bb7-64173dca5827"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function InferenceCoreModel.make_predict_function.<locals>.predict_function_trained at 0x7f7a0e47cb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["     PassengerId  Survived\n","0            892         0\n","1            893         0\n","2            894         0\n","3            895         0\n","4            896         1\n","..           ...       ...\n","413         1305         0\n","414         1306         1\n","415         1307         0\n","416         1308         0\n","417         1309         0\n","\n","[418 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["def make_submission(kaggle_predictions):\n","    path=\"submission.csv\"\n","    kaggle_predictions.to_csv(path, index=False,header=True)\n","    print(f\"Submission exported to {path}\")\n","\n","make_submission(kaggle_predictions)\n","\n","# Assuming 'filename.txt' is the name of your file\n","files.download('/content/submission.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"iyRQGv3JxOlx","executionInfo":{"status":"ok","timestamp":1721674484058,"user_tz":-120,"elapsed":325,"user":{"displayName":"Paul Croiz","userId":"04554486747591643883"}},"outputId":"faabcf52-a937-4cc6-c24d-1ecf34a67e05"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Submission exported to submission.csv\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_58b8b223-1deb-4676-a328-65c0415e4cae\", \"submission.csv\", 2839)"]},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"psRMM2leztTk"},"execution_count":null,"outputs":[]}]}